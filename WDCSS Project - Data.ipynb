{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pickle\n",
    "import ast\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to convert the SQL file into a Python dataframe. Each step processes a small batch of the main data to make it computational feasable. Afterwards, the seperate files were combined to the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/splits\\split_0.sql: 3402it [02:56, 19.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/splits\\split_0.sql_batch_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/splits\\split_1.sql: 3402it [02:48, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/splits\\split_1.sql_batch_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/splits\\split_2.sql: 3402it [02:46, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/splits\\split_2.sql_batch_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/splits\\split_3.sql: 3402it [02:43, 20.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/splits\\split_3.sql_batch_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/splits\\split_4.sql: 3402it [02:46, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/splits\\split_4.sql_batch_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/splits\\split_5.sql: 3402it [02:39, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/splits\\split_5.sql_batch_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/splits\\split_6.sql: 3402it [02:45, 20.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/splits\\split_6.sql_batch_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/splits\\split_7.sql: 3402it [02:41, 21.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/splits\\split_7.sql_batch_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/splits\\split_8.sql: 3402it [02:39, 21.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/splits\\split_8.sql_batch_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/splits\\split_9.sql: 3409it [02:42, 20.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/splits\\split_9.sql_batch_0.csv\n"
     ]
    }
   ],
   "source": [
    "def process_split_file(split_file_path, batch_size=1000, save_interval=10000):\n",
    "    \"\"\"\n",
    "    Processes a SQL dump file containing INSERT INTO statements, splits the data into batches, \n",
    "    and saves each batch to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        split_file_path (str): Path to the SQL dump file.\n",
    "        batch_size (int, optional): Number of rows per batch to process into a DataFrame. Default is 1000.\n",
    "        save_interval (int, optional): Number of batches to concatenate and save into a single CSV file. Default is 10000.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize counters and data storage\n",
    "    batch_count = 0  # Tracks the number of batches processed since the last save\n",
    "    file_count = 0   # Tracks the number of saved CSV files\n",
    "    batch_dfs = []   # List to store DataFrames for concatenation\n",
    "    batch_data = []  # Temporary list to hold rows of data\n",
    "\n",
    "    # Open the SQL dump file for reading\n",
    "    with open(split_file_path, \"r\", encoding=\"utf-8\") as sql_file:\n",
    "        # Read the file line by line with a progress bar\n",
    "        for line in tqdm(sql_file, desc=f\"Processing {split_file_path}\"):\n",
    "            # Check if the line starts with an INSERT INTO statement\n",
    "            if line.strip().startswith(\"INSERT INTO\"):\n",
    "                # Extract the values part of the INSERT statement\n",
    "                values = line.split(\"VALUES\")[1].strip().strip(\";\")\n",
    "                # Split the values into individual rows\n",
    "                rows = values.split(\"),(\"\")\n",
    "                for row in rows:\n",
    "                    # Clean up each row and split into individual values\n",
    "                    cleaned_row = row.strip(\"()\")\n",
    "                    batch_data.append([value.strip().strip(\"'\") for value in cleaned_row.split(\",\")])\n",
    "\n",
    "            # If the batch size is reached, process the data\n",
    "            if len(batch_data) >= batch_size:\n",
    "                # Convert the batch data into a DataFrame\n",
    "                batch_df = pd.DataFrame(batch_data, columns=[\"NodeID\", \"pl_from_namespace\", \"Links\"])\n",
    "                batch_dfs.append(batch_df)\n",
    "                batch_data = []  # Reset batch data\n",
    "                batch_count += 1\n",
    "\n",
    "                # Save data to a CSV file if the save interval is reached\n",
    "                if batch_count >= save_interval:\n",
    "                    full_df = pd.concat(batch_dfs, ignore_index=True)\n",
    "                    save_path = f\"{split_file_path}_batch_{file_count}.csv\"\n",
    "                    full_df.to_csv(save_path, index=False)\n",
    "                    batch_dfs = []  # Reset DataFrame storage\n",
    "                    batch_count = 0  # Reset batch counter\n",
    "                    file_count += 1  # Increment file counter\n",
    "\n",
    "    # Process any remaining data after the loop\n",
    "    if batch_data:\n",
    "        batch_df = pd.DataFrame(batch_data, columns=[\"NodeID\", \"pl_from_namespace\", \"Links\"])\n",
    "        batch_dfs.append(batch_df)\n",
    "\n",
    "    # Save any remaining DataFrames to a CSV file\n",
    "    if batch_dfs:\n",
    "        full_df = pd.concat(batch_dfs, ignore_index=True)\n",
    "        save_path = f\"{split_file_path}_batch_{file_count}.csv\"\n",
    "        full_df.to_csv(save_path, index=False)\n",
    "        print(f\"Saved {save_path}\")\n",
    "\n",
    "# Process each split file\n",
    "for split_file in split_files:\n",
    "    process_split_file(split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame saved to data/final_pagelinks.csv\n"
     ]
    }
   ],
   "source": [
    "def combine_processed_files(split_folder, output_file):\n",
    "    \"\"\"\n",
    "    Combines all processed CSV files in a folder into a single CSV file.\n",
    "\n",
    "    Args:\n",
    "        split_folder (str): Path to the folder containing processed CSV files.\n",
    "        output_file (str): Path to save the combined CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    all_dfs = []  # List to store DataFrames\n",
    "\n",
    "    # Traverse the folder to find all CSV files\n",
    "    for root, _, files in os.walk(split_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "                all_dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames and save to a single CSV\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Final DataFrame saved to {output_file}\")\n",
    "\n",
    "combine_processed_files(\"data/final_pagelinks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For preprocessing the raw data and enhance computational feasability different steps were performed. First only articles with type = 0 were needed and unnessecary columns removed. Furthermore, self-loops, dead ends and orphaned nodes were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "final_df = pd.read_csv(\"data/final_pagelinks.csv\")\n",
    "\n",
    "# Filter out non-articles\n",
    "df = final_df[final_df[\"pl_from_namespace\"] == 0]\n",
    "\n",
    "# Save preprocessing step\n",
    "df.to_csv(\"data/reduced_final_pagelinks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the namespace column\n",
    "tight_df = df.drop(columns=\"pl_from_namespace\")\n",
    "\n",
    "# Save preprocessing step\n",
    "tight_df.to_csv(\"data/reduced_final_pagelinks2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove self-loops for complexity reduction\n",
    "no_self_loops = tight_df[tight_df[\"NodeID\"] != tight_df[\"Links\"]]\n",
    "\n",
    "# Save preprocessing step\n",
    "no_self_loops.to_csv(\"data/no_self_loops_pagelinks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "no_self_loops = pd.read_csv(\"data/no_self_loops_pagelinks.csv\")\n",
    "\n",
    "# Get set of node and link IDs\n",
    "node_ids = set(no_self_loops[\"NodeID\"])\n",
    "link_ids = set(no_self_loops[\"Links\"])\n",
    "\n",
    "# Filter dead ends\n",
    "dead_ends = node_ids - link_ids\n",
    "\n",
    "# Remove dead ends\n",
    "no_dead_ends = no_self_loops[~no_self_loops[\"NodeID\"].isin(dead_ends)]\n",
    "\n",
    "# Save preprocessing step\n",
    "no_dead_ends.to_csv(\"data/no_dead_ends.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual set of node and link IDs\n",
    "node_ids = set(no_dead_ends[\"NodeID\"])\n",
    "link_ids = set(no_dead_ends[\"Links\"])\n",
    "\n",
    "# Filter orphaned nodes\n",
    "orphaned_nodes = link_ids - node_ids\n",
    "\n",
    "# Remove orphaned nodes\n",
    "no_orphaned_nodes = no_dead_ends[~no_dead_ends[\"Links\"].isin(orphaned_nodes)]\n",
    "\n",
    "# Save preprocessing step\n",
    "no_orphaned_nodes.to_csv(\"data/no_orphaned_nodes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group links by NodeID in a list\n",
    "grouped_df = no_orphaned_nodes.groupby(\"NodeID\", as_index=False).agg({\"Links\": list})\n",
    "\n",
    "# Save preprocessing step\n",
    "grouped_df.to_csv(\"data/grouped_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finale dataframe is used to built the Wikipedia Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done\n",
      "Convert data done\n"
     ]
    }
   ],
   "source": [
    "# Load graph data\n",
    "graph_df = pd.read_csv(\"data/grouped_df.csv\", index_col=None)\n",
    "print(\"Load data done\")\n",
    "\n",
    "# Convert lists in Links to int\n",
    "graph_df[\"Links\"] = graph_df[\"Links\"].apply(lambda x: [int(item) for item in ast.literal_eval(x)])\n",
    "print(\"Convert data done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3770339/3770339 [03:26<00:00, 18239.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Small Graph -----\n",
      "Number of Nodes smaller Graph: 4520178\n",
      "Number of Edges smaller Graph: 101881676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the graph\n",
    "G = nx.DiGraph() # Directed Graph\n",
    "\n",
    "# Add nodes and edges from the preprocessed dataset\n",
    "for _, row in tqdm(graph_df.iterrows(), total=len(graph_df)):\n",
    "    node = row[\"NodeID\"]\n",
    "    links = row[\"Links\"]\n",
    "    G.add_node(node)\n",
    "    for link in links:\n",
    "        G.add_edge(link, node)\n",
    "\n",
    "# Plot descriptive statistics about the graph\n",
    "print(\"----- Small Graph -----\")\n",
    "print(f\"Number of Nodes smaller Graph: {G.number_of_nodes()}\")\n",
    "print(f\"Number of Edges smaller Graph: {G.number_of_edges()}\\n\")\n",
    "\n",
    "# Save graph\n",
    "with open(\"data/en_wiki_graph.gpickle\", \"wb\") as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation of the graph strucutre the Giant Component Ratio was calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giant Component Ratio (GCR): 0.4824\n"
     ]
    }
   ],
   "source": [
    "# Calculate Giant Component Ratio (GCR)\n",
    "largest_component_size = len(max(nx.strongly_connected_components(G), key=len))\n",
    "total_nodes = G.number_of_nodes()\n",
    "GCR = largest_component_size / total_nodes\n",
    "\n",
    "print(f\"Giant Component Ratio (GCR): {GCR:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of \"Uni-Konstanz\"-Graph for ML-tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the ID of the article \"University of Constance\" was determine. Second the connected component to this article was identified and used for ML tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID for 'University of Konstanz': 2562056\n"
     ]
    }
   ],
   "source": [
    "def fetch_wikipedia_article_id(title, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Fetch the article ID of a Wikipedia page given its title using the Wikipedia API.\n",
    "    \n",
    "    Parameters:\n",
    "        title (str): The title of the Wikipedia article.\n",
    "        lang (str): The language code for the Wikipedia API (default is 'en').\n",
    "        \n",
    "    Returns:\n",
    "        int: The article ID of the Wikipedia page, or None if not found.\n",
    "    \"\"\"\n",
    "    # Construct the URL for the Wikipedia API\n",
    "    url = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "        \n",
    "        # Extract the page ID\n",
    "        for page_id, page_info in pages.items():\n",
    "            if page_id != \"-1\":\n",
    "                return int(page_id)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching article ID for title '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Get the article ID for the University of Konstanz\n",
    "title = \"University of Konstanz\"\n",
    "article_id = fetch_wikipedia_article_id(title)\n",
    "print(f\"Article ID for '{title}': {article_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Graph: 4520178 nodes, 101881676 edges\n",
      "Subgraph: 4515338 nodes, 101879136 edges\n"
     ]
    }
   ],
   "source": [
    "# Specify the target article ID to fetch its links\n",
    "target_node = 2562056\n",
    "\n",
    "# Find the connected component of the target node (in an undirected version of the graph)\n",
    "connected_nodes = nx.node_connected_component(G.to_undirected(), target_node)\n",
    "    \n",
    "# Create a subgraph from the connected nodes\n",
    "subgraph = G.subgraph(connected_nodes)\n",
    "    \n",
    "# Print information about the original graph and the subgraph\n",
    "print(f\"Original Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "print(f\"Subgraph: {subgraph.number_of_nodes()} nodes, {subgraph.number_of_edges()} edges\")\n",
    "\n",
    "# Save the full graph to a file in pickle format\n",
    "with open(\"data/kn_graph.gpickle\", \"wb\") as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of dataframe for features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the shortest paths and graph-based metrics, serving as features, were calculated and stored in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph using NetworkX\n",
    "with open(\"data/kn_graph.gpickle\", \"rb\") as f:\n",
    "    subgraph = pickle.load(f)\n",
    "\n",
    "print(\"----- Uni-KN Graph -----\")\n",
    "print(f\"Number of Nodes smaller Graph: {subgraph.number_of_nodes()}\")\n",
    "print(f\"Number of Edges smaller Graph: {subgraph.number_of_edges()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or initialize the DataFrame\n",
    "try:\n",
    "    # Try loading the DataFrame if it exists\n",
    "    shortest_path_df = pd.read_csv(\"data/shortest_path_df.csv\")\n",
    "    print(\"Loaded existing DataFrame.\")\n",
    "except FileNotFoundError:\n",
    "    # Initialize an empty DataFrame if not found\n",
    "    shortest_path_df = pd.DataFrame(columns=[\"source\", \"distance\"])\n",
    "    print(\"Initialized new DataFrame.\")\n",
    "\n",
    "# Assume G is your NetworkX graph\n",
    "uni_id = 2562056  # Target node ID for Lake Constance\n",
    "\n",
    "# Compute shortest paths if not already done\n",
    "if \"distance\" not in shortest_path_df.columns:\n",
    "    print(\"Computing shortest paths...\")\n",
    "    shortest_paths = {}\n",
    "    for node in tqdm(subgraph.nodes, desc=\"Computing shortest paths\"):\n",
    "        if node != uni_id:  # Exclude self-loop\n",
    "            try:\n",
    "                distance = nx.shortest_path_length(subgraph, source=node, target=uni_id)\n",
    "                shortest_paths[node] = distance\n",
    "            except nx.NetworkXNoPath:\n",
    "                pass  # Skip nodes with no path to Lake Constance\n",
    "\n",
    "    # Append shortest paths to DataFrame\n",
    "    data = [{\"source\": node, \"distance\": distance} for node, distance in shortest_paths.items()]\n",
    "    shortest_path_df = pd.DataFrame(data)\n",
    "    shortest_path_df.to_csv(\"data/shortest_path_df.csv\", index=False)\n",
    "    print(\"Shortest paths saved.\")\n",
    "\n",
    "# PageRank\n",
    "if \"pagerank_source\" not in shortest_path_df.columns:\n",
    "    print(\"Computing PageRank...\")\n",
    "    pagerank_scores = nx.pagerank(subgraph)\n",
    "    shortest_path_df[\"pagerank_source\"] = shortest_path_df[\"source\"].map(pagerank_scores)\n",
    "    shortest_path_df.to_csv(\"data/shortest_path_df.csv\", index=False)\n",
    "    print(\"PageRank scores saved.\")\n",
    "\n",
    "# In-Degree\n",
    "if \"in_degree_source\" not in shortest_path_df.columns:\n",
    "    print(\"Computing In-Degree...\")\n",
    "    in_degrees = dict(subgraph.in_degree())\n",
    "    shortest_path_df[\"in_degree_source\"] = shortest_path_df[\"source\"].map(in_degrees)\n",
    "    shortest_path_df.to_csv(\"data/shortest_path_df.csv\", index=False)\n",
    "    print(\"In-Degrees saved.\")\n",
    "\n",
    "# Out-Degree\n",
    "if \"out_degree_source\" not in shortest_path_df.columns:\n",
    "    print(\"Computing Out-Degree...\")\n",
    "    out_degrees = dict(subgraph.out_degree())\n",
    "    shortest_path_df[\"out_degree_source\"] = shortest_path_df[\"source\"].map(out_degrees)\n",
    "    shortest_path_df.to_csv(\"data/shortest_path_df.csv\", index=False)\n",
    "    print(\"Out-Degrees saved.\")\n",
    "\n",
    "# HITS\n",
    "if \"hub_score_source\" not in shortest_path_df.columns or \"authority_score_source\" not in shortest_path_df.columns:\n",
    "    print(\"Computing HITS...\")\n",
    "    hubs, authorities = nx.hits(subgraph)\n",
    "    shortest_path_df[\"hub_score_source\"] = shortest_path_df[\"source\"].map(hubs)\n",
    "    shortest_path_df[\"authority_score_source\"] = shortest_path_df[\"source\"].map(authorities)\n",
    "    shortest_path_df.to_csv(\"data/shortest_path_df.csv\", index=False)\n",
    "    print(\"HITS scores saved.\")\n",
    "\n",
    "# Eigenvector Centrality\n",
    "if \"eigenvector_centrality\" not in shortest_path_df.columns:\n",
    "    print(\"Computing Eigenvector Centrality...\")\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(subgraph, max_iter=100, tol=1e-06)\n",
    "    shortest_path_df[\"eigenvector_centrality\"] = shortest_path_df[\"source\"].map(eigenvector_centrality)\n",
    "    shortest_path_df.to_csv(\"data/shortest_path_df.csv\", index=False)\n",
    "    print(\"Eigenvector Centrality saved.\")\n",
    "\n",
    "# Betweenness Centrality\n",
    "if \"betweenness_centrality\" not in shortest_path_df.columns:\n",
    "    print(\"Computing Betweenness Centrality...\")\n",
    "    betweenness_centrality = nx.betweenness_centrality(subgraph, k=400, normalized=True)\n",
    "    shortest_path_df[\"betweenness_centrality\"] = shortest_path_df[\"source\"].map(betweenness_centrality)\n",
    "    shortest_path_df.to_csv(\"data/shortest_path_df.csv\", index=False)\n",
    "    print(\"Betweenness Centrality saved.\")\n",
    "\n",
    "shortest_path_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
